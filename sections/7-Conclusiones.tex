\section{Conclusiones y trabajo futuro}\label{sec:conclusiones}

Hemos preprocesado los datos, experimentando con diferentes métodos de preprocesado y distintos modelos. Una vez hemos obtenido los mejores resultados, también hemos probado a refinar los modelos haciendo \textit{hyperparameter tuning}. Por último, también hemos probado a entrenar los modelos aumentando los datos de entrenamiento, generándolos con técnicas de \textit{data augmentation}.

Hemos cumplido con los objetivos marcados del proyecto, aprendiendo las bases del \gls{ml} y aplicándolas a un problema real, siendo capaces de detectar granos contaminados en las imágenes \gls{bil} de forma automática.

Con los resultados que hemos obtenido, podemos afirmar que hemos desarrollado un sistema habilitado para aplicarse de forma eficaz sobre una selección de trigo, pues el tiempo de procesado y predicción son de una magnitud inferior al segundo. Según las pruebas sobre $42$ archivos \gls{bil}, de media el procesado y la predicción de cada archivo ha tardado $960$ y de $7$ milisegundos respectivamente. Aunque estos valores nos sirvan para una aplicación para el laboratorio, no son excesivamente rápidos para una aplicación real, por lo que se debería de mejorar la eficiencia del sistema empezando por habilitar una aplicación sin \textit{GUI} para evitarnos la parte de renderizar las imágenes en la interfaz gráfica, la utilización de la \textit{GPU} a la hora de preprocesar los datos, etc. 

A medida que hemos ido probando diferentes metodologías a lo largo del proyecto, hemos visto la importancia de la elección tanto de las herramientas de preprocesado como de los modelos. Pero hay decisiones que hubiera tomado de forma diferente si volviera a empezar el proyecto, las comento a continuación:

\begin{enumerate}
    \item \textbf{Mejores modelos}: Una posible mejora que no hemos utilizado en este proyecto, en parte porque el objetivo que tenía con el era aprender las bases del \gls{ml} utilizando \textit{sklearn}, es la utilización de redes neuronales, las cuales suelen funcionar mejor (aunque probablemente tuvieramos los mismos problemas de falta de datos).
    \item \textbf{Entorno de desarrollo}: La utilización de \textit{notebooks}, pues me hubieran permitido un mayor control sobre la ejecución del código durante las pruebas y agilizar el proceso de entrenamiento. Nos los hemos utilizado en este proyecto porque queríamos compartir el módulo de preprocesado de datos con la aplicación de escritorio, sin embargo se podría haber hecho de otra forma.
    \item \textbf{\textit{Hyperparameter tuning}}: La utilización de \textit{Optuna} para la búsqueda de hiperparámetros, pues es una librería que permite la búsqueda de hiperparámetros de forma más eficiente que el \textit{RandomSearch} que hemos utilizado.
    \item \textbf{Preprocesado}: En el paso de detección de \textit{outliers} podríamos en lugar de eliminar los granos con valores atípicos, interpolar esos valores para no perder demasiados datos ni trabajar con datos ``crudos''.
    \item \textbf{Modelos \textit{ensemble}}: La utilización de modelos de votación \textit{soft} con los parámetros que hemos obtenido en el \textit{hyperparameter tuning}, para ver si se obtienen mejores resultados.
    \item \textbf{Enfoque}: A lo largo de los diferentes experimentos, casi siempre hemos obtenido buenos resultados con el modelo \textit{K-Neighbors}, por lo que tal vez hubiera sido mejor enfocar más los esfuerzos en este modelo y no tratar de encontrar los mejores pasos de preprocesado para todos los modelos comparando las medias. Por ejemplo, podemos ver en la \textit{Tabla\ \ref{tab:nopreprocessing-derivative-results}} que el modelo ya obtiene buenos resultados sin siquiera preprocesar los datos. Por lo tanto, podríamos haber probado a enfocarnos en encontrar pasos de preprocesado que fueran mejores para \textit{K-Neighbors} en específico y no en general.
\end{enumerate}

Por último, a nivel personal el proyecto ha sido un paso importante, pues antes de empezarlo, pese a tener algunos conceptos básicos interiorizados, no había visto todavía la aplicación de estos conceptos en un problema real. Me ha parecido muy interesante el proceso de exploración y el descubrimiento de formas de tratar los datos que desconocía, como por ejemplo la reducción de dimensionalidad. Además de la parte de \gls{ml}, también el desarrollo y compilación de la aplicación de escritorio ha supuesto un aprendizaje, pues nunca había hecho un proyecto parecido que tuviera una interfaz gráfica en \textit{Python}. Gracias a este proyecto, he decidido enfocar mis próximos proyectos personales en el campo del \gls{ml} y \textit{Deep Learning}, pues me parece un campo muy interesante y con muchas posibilidades del que me interesaría aprender más, sobretodo de \textit{Deep Learning}, viendo de primera mano la aplicabilidad que tienen las técnicas que hemos aprendido al mercado laboral.